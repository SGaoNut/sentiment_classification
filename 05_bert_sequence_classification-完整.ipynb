{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT情感分析\n",
    "\n",
    "![jupyter](./imgs/bert_classification.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 创建数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>在 韩 红 基 金 会 0 0 0 0 年 的 审 计 报 告 上 可 看 到 其 0 0 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>感 谢 有 你 们 向 所 有 奋 战 在 一 线 的 医 护 人 员 致 敬 期 待 大 ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0 0 省 份 一 省 包 一 市 支 援 湖 北 嘿 你 在 干 嘛 呢 何 老 师 的 ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>普 通 感 冒 以 后 也 别 吃 抗 生 素 了 烧 吃 退 烧 药 布 洛 芬 啥 的 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>爷 爷 你 好 我 是 武 汉 儿 童 医 院 的 护 士 你 带 着 小 孩 来 武 汉 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  在 韩 红 基 金 会 0 0 0 0 年 的 审 计 报 告 上 可 看 到 其 0 0 ...      1\n",
       "1  感 谢 有 你 们 向 所 有 奋 战 在 一 线 的 医 护 人 员 致 敬 期 待 大 ...      2\n",
       "2  0 0 省 份 一 省 包 一 市 支 援 湖 北 嘿 你 在 干 嘛 呢 何 老 师 的 ...      2\n",
       "3  普 通 感 冒 以 后 也 别 吃 抗 生 素 了 烧 吃 退 烧 药 布 洛 芬 啥 的 ...      1\n",
       "4  爷 爷 你 好 我 是 武 汉 儿 童 医 院 的 护 士 你 带 着 小 孩 来 武 汉 ...      1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('./dataset/train.csv')\n",
    "val_df = pd.read_csv('./dataset/val.csv')\n",
    "test_df = pd.read_csv('./dataset/test.csv')\n",
    "val_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Hugging Face Transformers \n",
    "\n",
    "Transformers提供了NLP领域大量state-of-art的预训练语言模型结构的模型和调用框架。  \n",
    "到目前为止，transformers 提供了超过100种语言的，32种预训练语言模型，简单，强大，高性能，是新手入门的不二选择。   \n",
    "\n",
    "![jupyter](./imgs/berts.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT 输入格式\n",
    "![jupyter](./imgs/bert_inputs.png)\n",
    "\n",
    "### BERT 文本分类输入\n",
    "![jupyter](./imgs/bert_for_classification.png)\n",
    "\n",
    "### 使用TFBertForSequenceClassification进行文本分类\n",
    "https://huggingface.co/transformers/model_doc/bert.html#tfbertforsequenceclassification   \n",
    "\n",
    "BERT 具有两种输出  \n",
    "1. pooler output，对应的[CLS]的输出   \n",
    "2. sequence output，对应的是序列中的所有字的最后一层hidden输出last_hidden_state。   \n",
    "\n",
    "BERT主要可以处理两种，\n",
    "- 一种任务是分类/回归任务（使用的是pooler output）\n",
    "- 一种是序列任务（sequence output）。  \n",
    "TFBertForSequenceClassification，即使用pooler output接softmax进行分类任务。  \n",
    "![jupyter](./imgs/bert_outputs.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# 定义中文base bert的tokenzier\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids\n",
      "[101, 1091, 1762, 2399, 3314, 1100, 1159, 2111, 2094, 3837, 2697, 4638, 5018, 758, 1921, 8024, 2769, 812, 793, 4197, 3766, 3300, 2563, 6381, 4178, 2658, 2881, 2849, 6821, 8439, 2399, 4638, 5018, 671, 1921, 511, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "token_type_ids\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "attention_mask\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "test_sentence = '写在年末冬初孩子流感的第五天，我们仍然没有忘记热情拥抱这2020年的第一天。'\n",
    "\n",
    "# 使用中文base bert的tokenzier将文本转化为对应bert的输入\n",
    "bert_input = tokenizer.encode_plus(\n",
    "                        test_sentence,                      \n",
    "                        add_special_tokens = True,  # 标记是否添加[CLS], [SEP]特殊字符\n",
    "                        max_length = 50, #  最长序列长度\n",
    "                        pad_to_max_length = True, # 标记是否添加[PAD]到最长长度\n",
    "                        truncation=True,  # 标记是否截断\n",
    "                        return_attention_mask = True, # 添加注意力掩码，使注意力计算不关注pad的数据\n",
    "                        )\n",
    "\n",
    "for k, v in bert_input.items():\n",
    "    print(k)\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_sample_to_feature(text, max_length):\n",
    "    return tokenizer.encode_plus(text, \n",
    "                                 add_special_tokens=True, \n",
    "                                 max_length=max_length, \n",
    "                                 padding='max_length',    \n",
    "                                 truncation=True,\n",
    "                                 return_attention_mask = True,\n",
    "                                )\n",
    "\n",
    "# 将输入映射成TFBertForSequenceClassification的格式\n",
    "def map_sample_to_dict(input_ids, token_type_ids, attention_masks, label):\n",
    "    return {\n",
    "      \"input_ids\": input_ids,\n",
    "      \"token_type_ids\": token_type_ids,\n",
    "      \"attention_mask\": attention_masks,\n",
    "  }, label\n",
    "\n",
    "\n",
    "# 创建TF数据集\n",
    "def build_dataset(df, max_length):\n",
    "    # 准备列表，以便我们可以从列表中构建最终的TensorFlow数据集\n",
    "    input_ids_list = []\n",
    "    token_type_ids_list = []\n",
    "    attention_mask_list = []\n",
    "    label_list = []\n",
    "    # 将输入数据转化为BERT输入\n",
    "    for _, row in df.iterrows():\n",
    "        text, label = row[\"text\"], row[\"label\"]\n",
    "        bert_input = convert_sample_to_feature(text, max_length)  # 对文本进行转换成BERT输入\n",
    "        input_ids_list.append(bert_input['input_ids'])\n",
    "        token_type_ids_list.append(bert_input['token_type_ids'])\n",
    "        attention_mask_list.append(bert_input['attention_mask'])\n",
    "        label_list.append([label])\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((input_ids_list, attention_mask_list, token_type_ids_list, label_list))\n",
    "    dataset = dataset.map(map_sample_to_dict)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32   # bert模型较复杂参数较多，batch size一般不大\n",
    "MAX_SEQ_LEN = 240  # 最长序列长度\n",
    "NUM_LABELS = 3  # 标签数量\n",
    "BUFFER_SIZE = len(train_df)\n",
    "\n",
    "# 创建数据集\n",
    "train_dataset = build_dataset(train_df, MAX_SEQ_LEN).shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(BUFFER_SIZE)\n",
    "val_dataset = build_dataset(val_df, MAX_SEQ_LEN).batch(BATCH_SIZE)\n",
    "test_dataset = build_dataset(test_df, MAX_SEQ_LEN).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 构建BERT分类模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFBertForSequenceClassification\n",
    "\n",
    "# 使用TF版本的中文base bert分类模型\n",
    "model = TFBertForSequenceClassification.from_pretrained('bert-base-chinese',  # base中文bert\n",
    "                                                        num_labels=NUM_LABELS  # 指定输出的类别数\n",
    "                                                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT学习率一般较小, 使用Adam优化器 3e-5, 3e-6\n",
    "LR = 3e-6\n",
    "\n",
    "# BERT参数量大，拟合能力较强，在这个数据集上不需要太多迭代\n",
    "EPOCHS = 5\n",
    "\n",
    "# 同样早停等待次数也设置小一些\n",
    "PATIENCE = 1\n",
    "\n",
    "# 常用Adam优化器\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=LR)\n",
    "\n",
    "# 这里的标签值并不是one-hot的，所以loss需要SparseCategoricalCrossentropy\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) # from_logits为True会用softmax将y_pred转化为概率，结果更稳定\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss=loss,\n",
    "              metrics=[metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.Socket(zmq.PUSH) at 0x7f13aa3a39a0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.Socket(zmq.PUSH) at 0x7f13aa3a39a0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "2338/2338 [==============================] - ETA: 0s - loss: 0.6598 - accuracy: 0.7110WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "2338/2338 [==============================] - 3568s 2s/step - loss: 0.6598 - accuracy: 0.7110 - val_loss: 0.6079 - val_accuracy: 0.7294\n",
      "Epoch 2/5\n",
      "2338/2338 [==============================] - 3570s 2s/step - loss: 0.5903 - accuracy: 0.7425 - val_loss: 0.5995 - val_accuracy: 0.7348\n",
      "Epoch 3/5\n",
      "2338/2338 [==============================] - 3569s 2s/step - loss: 0.5567 - accuracy: 0.7574 - val_loss: 0.5861 - val_accuracy: 0.7414\n",
      "Epoch 4/5\n",
      "2338/2338 [==============================] - 3571s 2s/step - loss: 0.5276 - accuracy: 0.7721 - val_loss: 0.5927 - val_accuracy: 0.7432\n",
      "Epoch 5/5\n",
      "2338/2338 [==============================] - 3570s 2s/step - loss: 0.4917 - accuracy: 0.7910 - val_loss: 0.6039 - val_accuracy: 0.7446\n"
     ]
    }
   ],
   "source": [
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy',\n",
    "                                            patience=PATIENCE,\n",
    "                                            restore_best_weights=True)\n",
    "\n",
    "bert_history = model.fit(train_dataset,\n",
    "                         epochs=EPOCHS,\n",
    "                         callbacks=[callback],\n",
    "                         validation_data=val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:From /home/teacher/.local/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From /home/teacher/.local/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "INFO:tensorflow:Assets written to: ./bert/bert_classification/saved_model/1/assets\n"
     ]
    }
   ],
   "source": [
    "# BERT模型保存\n",
    "save_model_path = \"./bert/bert_classification\"\n",
    "model.save_pretrained(save_model_path, saved_model=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 模型评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TFSequenceClassifierOutput(loss=None, logits=array([[-2.827365  ,  0.62048835,  2.1867077 ],\n",
       "       [-2.3731406 ,  0.91308105,  1.669932  ],\n",
       "       [ 1.601606  ,  0.28248912, -1.983878  ],\n",
       "       ...,\n",
       "       [-2.8215024 ,  2.8636997 , -0.22697781],\n",
       "       [ 1.6240071 ,  0.98691046, -2.6499858 ],\n",
       "       [-2.706423  , -0.49490097,  3.3001475 ]], dtype=float32), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 结果包括loss和logits, 取出模型预测Logits\n",
    "output = model.predict(test_dataset)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 0, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = np.argmax(output.logits, axis=-1)\n",
    "\n",
    "preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.66      0.67      1796\n",
      "           1       0.78      0.81      0.79      5651\n",
      "           2       0.74      0.68      0.71      2551\n",
      "\n",
      "    accuracy                           0.75      9998\n",
      "   macro avg       0.73      0.72      0.72      9998\n",
      "weighted avg       0.75      0.75      0.75      9998\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "test_label = test_df['label']\n",
    "result = classification_report(test_label, preds)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 模型预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at ./bert/bert_classification were not used when initializing TFBertForSequenceClassification: ['dropout_37']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at ./bert/bert_classification.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  102267648 \n",
      "_________________________________________________________________\n",
      "dropout_75 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  2307      \n",
      "=================================================================\n",
      "Total params: 102,269,955\n",
      "Trainable params: 102,269,955\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 加载保存好的模型\n",
    "save_model_path = \"./bert/bert_classification\"\n",
    "saved_model = TFBertForSequenceClassification.from_pretrained(save_model_path, \n",
    "                                                              num_labels=NUM_LABELS)\n",
    "saved_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.436507  , -0.08783254, -0.18646817],\n",
       "       [-1.7852176 ,  0.88021284,  1.3777912 ],\n",
       "       [-2.0149825 , -0.03028507,  2.2247462 ]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentences = [\n",
    "    \"因为疫情被困家里2个月了，好压抑啊，感觉自己抑郁了！\",\n",
    "    \"我国又一个新冠病毒疫苗获批紧急使用。\",\n",
    "    \"我们在一起，打赢这场仗，抗击新馆疫情，我们在行动！\"]\n",
    "\n",
    "# 调用中文bert base模型的tokenzier\n",
    "predict_inputs = tokenizer(predict_sentences,\n",
    "                           padding=True,\n",
    "                           max_length=MAX_SEQ_LEN, \n",
    "                           return_tensors=\"tf\")\n",
    "# 直接call保存好的bert model\n",
    "output = saved_model(predict_inputs)\n",
    "\n",
    "# 取出模型预测结果的logits\n",
    "predict_logits = output.logits.numpy()\n",
    "\n",
    "predict_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1, 1, 1]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 取出分数最高的标签\n",
    "predict_results = np.argmax(predict_logits, axis=1)\n",
    "# 还原标签\n",
    "predict_labels = [label - 1 for label in predict_results] \n",
    "predict_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文本: 因为疫情被困家里2个月了，好压抑啊，感觉自己抑郁了！\n",
      "预测标签: -1\n",
      "文本: 我国又一个新冠病毒疫苗获批紧急使用。\n",
      "预测标签: 1\n",
      "文本: 我们在一起，打赢这场仗，抗击新馆疫情，我们在行动！\n",
      "预测标签: 1\n"
     ]
    }
   ],
   "source": [
    "# 格式化预测结果\n",
    "for text, label in zip(predict_sentences, predict_labels):\n",
    "    print(f'文本: {text}\\n预测标签: {label}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 模型优化\n",
    "\n",
    "BERT是一种预训练语言模型，参数量较大，训练较慢。    \n",
    "可以将BERT作为embedding层，固定其参数，只做前向运算，再接其他特征抽取层进行特征抽取。\n",
    "\n",
    "#### 将BERT作为embedding层  \n",
    "![jupyter](./imgs/bert_embedding.png)\n",
    "\n",
    "#### BERT接特征抽取层\n",
    "![jupyter](./imgs/bert_embedding2.png)\n",
    "\n",
    "#### 使用BERT的序列输出\n",
    "![jupyter](./imgs/bert_token_classification.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-chinese were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-chinese.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFBertModel\n",
    "\n",
    "bert_model = TFBertModel.from_pretrained('bert-base-chinese')  # 初始化中文bert base model\n",
    "\n",
    "input_ids = tf.keras.layers.Input(shape=(MAX_SEQ_LEN,), name='input_ids', dtype='int32')\n",
    "token_type_ids = tf.keras.layers.Input(shape=(MAX_SEQ_LEN,), name='token_type_ids', dtype='int32')  # 定义bert model输入\n",
    "attention_masks = tf.keras.layers.Input(shape=(MAX_SEQ_LEN,), name='attention_mask', dtype='int32') \n",
    "embedding_layer = bert_model(input_ids, attention_masks)[0]  # 取出BERT另一种输出last_hidden_state，然后特征抽取器\n",
    "X = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(100,  # 使用双向LSTM进行特征抽取\n",
    "                                                       return_sequences=True,\n",
    "                                                       dropout=0.1))(embedding_layer)\n",
    "X = tf.keras.layers.GlobalMaxPool1D()(X)  # 进行max pooling\n",
    "X = tf.keras.layers.BatchNormalization()(X) \n",
    "X = tf.keras.layers.Dense(256, activation='relu')(X)\n",
    "X = tf.keras.layers.Dropout(0.5)(X)\n",
    "y = tf.keras.layers.Dense(3, activation='softmax', name='outputs')(X)  # 3 labels due to three sentiment classes\n",
    "\n",
    "model = tf.keras.Model(inputs=[input_ids, attention_masks, token_type_ids], outputs = y)\n",
    "\n",
    "for layer in model.layers[:3]:  # 将BERT相关层权重冻结，不可训练，只做前向运算\n",
    "     layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 240)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "attention_mask (InputLayer)     [(None, 240)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_bert_model (TFBertModel)     TFBaseModelOutputWit 102267648   input_ids[0][0]                  \n",
      "                                                                 attention_mask[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional (Bidirectional)   (None, 240, 200)     695200      tf_bert_model[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d (GlobalMax (None, 200)          0           bidirectional[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 200)          800         global_max_pooling1d[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 256)          51456       batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dropout_113 (Dropout)           (None, 256)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "token_type_ids (InputLayer)     [(None, 240)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "outputs (Dense)                 (None, 3)            771         dropout_113[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 103,015,875\n",
      "Trainable params: 747,827\n",
      "Non-trainable params: 102,268,048\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}