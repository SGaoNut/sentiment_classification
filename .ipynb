{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.中文分词\n",
    "### https://github.com/fxsjy/jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.685 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default Mode: 武汉/ 加油/ ！/ 中国/ 加油/ ！/ 没有/ 一个/ 冬天/ 不可逾越/ ，/ 没有/ 一个/ 春天/ 不会/ 来临/ 。\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "\n",
    "seg_list = jieba.cut(\"武汉加油！中国加油！没有一个冬天不可逾越，没有一个春天不会来临。\", cut_all=False)\n",
    "print(\"Default Mode: \" + \"/ \".join(seg_list))  # 精确模式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.词频向量化\n",
    "### CountVectorizer 类会将文本中的词语转换为词频矩阵\n",
    "### https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 8)\t1\n",
      "  (0, 9)\t1\n",
      "  (0, 0)\t1\n",
      "  (0, 7)\t1\n",
      "  (0, 6)\t1\n",
      "  (1, 8)\t1\n",
      "  (1, 7)\t1\n",
      "  (1, 4)\t1\n",
      "  (1, 5)\t1\n",
      "  (1, 10)\t1\n",
      "  (2, 7)\t1\n",
      "  (2, 2)\t1\n",
      "  (2, 1)\t1\n",
      "  (3, 7)\t1\n",
      "  (3, 3)\t1\n",
      "['一起', '举行', '军运会', '加油', '可怕', '新冠', '樱花', '武汉', '疫情', '结束', '蔓延']\n",
      "1\t0\t0\t0\t0\t0\t1\t1\t1\t1\t0\n",
      "0\t0\t0\t0\t1\t1\t0\t1\t1\t0\t1\n",
      "0\t1\t1\t0\t0\t0\t0\t1\t0\t0\t0\n",
      "0\t0\t0\t1\t0\t0\t0\t1\t0\t0\t0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "corpus = ['疫情结束，一起去武汉看樱花！',\n",
    "          '可怕！新冠疫情从武汉蔓延！',\n",
    "          '军运会在武汉举行。',\n",
    "          '武汉加油！',\n",
    "        ]\n",
    "\n",
    "tokenzied_corpus = [' '.join(jieba.cut(sentence, cut_all=False)) for sentence in corpus]\n",
    "\n",
    "vectorizer = CountVectorizer(min_df=1)\n",
    "X = vectorizer.fit_transform(tokenzied_corpus)\n",
    "feature_name = vectorizer.get_feature_names()\n",
    "\n",
    "print (X)\n",
    "print (feature_name)\n",
    "for w in X.toarray():\n",
    "    print('\\t'.join([str(round(x, 2)) for x in w]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.TF-IDF处理\n",
    "\n",
    "### 然而有些词在文本中尽管词频高，但是并不重要，这个时候就可以用TF-IDF技术。某一特定文件内的高词语频率，以及该词语在整个文件集合中的低文件频率，可以产生出高权重的TF-IDF。因此，TF-IDF倾向于过滤掉常见的词语，保留重要的词语。   \n",
    "### https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer \n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "pipe = Pipeline([('count', CountVectorizer()),\n",
    "                 ('tfid', TfidfTransformer())]).fit(tokenzied_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "featrure = pipe.transform(tokenzied_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['一起', '举行', '军运会', '加油', '可怕', '新冠', '樱花', '武汉', '疫情', '结束', '蔓延']\n",
      "\n",
      "1.92\t1.92\t1.92\t1.92\t1.92\t1.92\t1.92\t1.0\t1.51\t1.92\t1.92\n",
      "\n",
      "0.51\t0.0\t0.0\t0.0\t0.0\t0.0\t0.51\t0.26\t0.4\t0.51\t0.0\n",
      "0.0\t0.0\t0.0\t0.0\t0.51\t0.51\t0.0\t0.26\t0.4\t0.0\t0.51\n",
      "0.0\t0.66\t0.66\t0.0\t0.0\t0.0\t0.0\t0.35\t0.0\t0.0\t0.0\n",
      "0.0\t0.0\t0.0\t0.89\t0.0\t0.0\t0.0\t0.46\t0.0\t0.0\t0.0\n"
     ]
    }
   ],
   "source": [
    "weight = featrure.toarray()\n",
    "print(pipe['count'].get_feature_names())\n",
    "print()\n",
    "idfs = pipe['tfid'].idf_\n",
    "print('\\t'.join([str(round(x, 2)) for x in idfs]))\n",
    "print()\n",
    "for w in weight:\n",
    "    print('\\t'.join([str(round(x, 2)) for x in w]))\n",
    "\n",
    "#weight就是模型的输入，它的大小(m,n),m是文本个数，n是词库的大小"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. xgboost模型\n",
    "\n",
    "### https://xgboost.readthedocs.io/en/latest/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['--', '?', '“', '”', '》', '－－', 'able', 'about', 'above', 'according']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('./data/baidu_stopwords.txt', 'r') as f:\n",
    "    stop_words = f.read().split('\\n')\n",
    "\n",
    "stop_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'武汉 樱花 很漂亮'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cut_and_clean(text):\n",
    "    cuted_text = ' '.join([x for x in jieba.lcut(text) if x not in stop_words])\n",
    "    clean_text = re.sub('([\\.，。、“”‘ ’？\\?:#：【】\\+!！《 》「」『』\\{\\}\\|])', ' ', cuted_text)\n",
    "    clean_text = re.sub('\\s{2,}', ' ', clean_text)\n",
    "    clean_text = clean_text.lower()\n",
    "    return clean_text.strip()\n",
    "text = '武汉的樱花很漂亮！'\n",
    "cut_and_clean(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>肺炎疫情</td>\n",
       "      <td>0</td>\n",
       "      <td>肺炎 疫情</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>【西方记者挑事提问，WHO总干事用事实力挺中国】12日，世界卫生组织大会上，有西方记者挑事提...</td>\n",
       "      <td>0</td>\n",
       "      <td>西方 记者 挑事 提问 who 总干事 用事 实力 挺 中国 12 日 世界卫生组织 大会 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>人数终于 始呈下降趋势。这个春节有喜有忧，但事情终有完结的那一天。中午出去买了一次蔬菜，过了...</td>\n",
       "      <td>0</td>\n",
       "      <td>人数 终于 始 呈 下降 趋势 春节 有喜有忧 事情 终 完结 中午 买 蔬菜 六道 关卡 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>【抗病毒小刘上线】今天有好几个朋友来问我戴没戴口罩要不要寄口罩给我，超级感动，小刘除了被吓到...</td>\n",
       "      <td>0</td>\n",
       "      <td>抗病毒 小 刘 上线 好几个 朋友 问 戴 没 戴 口罩 寄 口罩 超级 感动 小 刘 惊慌...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>潜伏期传染，后期会不会更惊讶O最新疫情地图出炉，这传播速度有让你觉得惊讶吗</td>\n",
       "      <td>0</td>\n",
       "      <td>潜伏期 传染 后期 会 更 惊讶 o 最新 疫情 地图 出炉 传播速度 惊讶</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label  \\\n",
       "0                                               肺炎疫情      0   \n",
       "1  【西方记者挑事提问，WHO总干事用事实力挺中国】12日，世界卫生组织大会上，有西方记者挑事提...      0   \n",
       "2  人数终于 始呈下降趋势。这个春节有喜有忧，但事情终有完结的那一天。中午出去买了一次蔬菜，过了...      0   \n",
       "3  【抗病毒小刘上线】今天有好几个朋友来问我戴没戴口罩要不要寄口罩给我，超级感动，小刘除了被吓到...      0   \n",
       "4              潜伏期传染，后期会不会更惊讶O最新疫情地图出炉，这传播速度有让你觉得惊讶吗      0   \n",
       "\n",
       "                                      processed_text  \n",
       "0                                              肺炎 疫情  \n",
       "1  西方 记者 挑事 提问 who 总干事 用事 实力 挺 中国 12 日 世界卫生组织 大会 ...  \n",
       "2  人数 终于 始 呈 下降 趋势 春节 有喜有忧 事情 终 完结 中午 买 蔬菜 六道 关卡 ...  \n",
       "3  抗病毒 小 刘 上线 好几个 朋友 问 戴 没 戴 口罩 寄 口罩 超级 感动 小 刘 惊慌...  \n",
       "4             潜伏期 传染 后期 会 更 惊讶 o 最新 疫情 地图 出炉 传播速度 惊讶  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('./processed_data/train.csv', encoding='utf-8')\n",
    "train_df['processed_text'] = train_df['text'].apply(cut_and_clean)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>星火线上课 足不出户，同样进步！L星火教育呼市的 视频</td>\n",
       "      <td>1</td>\n",
       "      <td>星火 线 上课 足不出户 l 星火 教育 呼市 视频</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>中科院病毒研究所石正丽老师演讲《追踪SARS的源头》探索病毒与人类社会的关系。真是好看</td>\n",
       "      <td>1</td>\n",
       "      <td>中科院 病毒 研究所 石正丽 老师 演讲 追踪 sars 源头 探索 病毒 人类 社会 关系 好看</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>嗯 @熊中默就是那个给那位挂基层辅警照片 暴平安天门几万条艾滋病巨婴gay洗地还挂plmm照...</td>\n",
       "      <td>-1</td>\n",
       "      <td>@ 熊中默 那位 挂 基层 辅警 照片 暴 平安 天门 几万 条 艾滋病 巨婴 gay 洗地...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>今天看到两条让我真的笑，笑出声的 。「为防病毒，出门抢购双黄连，染上病毒。」O 「新冠病毒感...</td>\n",
       "      <td>-1</td>\n",
       "      <td>两条 真的 笑 笑 出声 防病毒 出门 抢购 双黄连 染上 病毒 o 新冠 病毒感染 肾脏 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@_莫西干_</td>\n",
       "      <td>0</td>\n",
       "      <td>@ _ 莫西 干 _</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label  \\\n",
       "0                        星火线上课 足不出户，同样进步！L星火教育呼市的 视频      1   \n",
       "1        中科院病毒研究所石正丽老师演讲《追踪SARS的源头》探索病毒与人类社会的关系。真是好看      1   \n",
       "2  嗯 @熊中默就是那个给那位挂基层辅警照片 暴平安天门几万条艾滋病巨婴gay洗地还挂plmm照...     -1   \n",
       "3  今天看到两条让我真的笑，笑出声的 。「为防病毒，出门抢购双黄连，染上病毒。」O 「新冠病毒感...     -1   \n",
       "4                                             @_莫西干_      0   \n",
       "\n",
       "                                      processed_text  \n",
       "0                         星火 线 上课 足不出户 l 星火 教育 呼市 视频  \n",
       "1  中科院 病毒 研究所 石正丽 老师 演讲 追踪 sars 源头 探索 病毒 人类 社会 关系 好看  \n",
       "2  @ 熊中默 那位 挂 基层 辅警 照片 暴 平安 天门 几万 条 艾滋病 巨婴 gay 洗地...  \n",
       "3  两条 真的 笑 笑 出声 防病毒 出门 抢购 双黄连 染上 病毒 o 新冠 病毒感染 肾脏 ...  \n",
       "4                                         @ _ 莫西 干 _  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv('./processed_data/test.csv', encoding='utf-8')\n",
    "test_df['processed_text'] = test_df['text'].apply(cut_and_clean)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_df['processed_text']\n",
    "train_label = train_df['label'].apply(lambda x: x+1).astype(int)\n",
    "\n",
    "test_data = test_df['processed_text']\n",
    "test_label = test_df['label'].apply(lambda x: x+1).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(max_df=5000, min_df=50)\n",
    "tfidftransformer = TfidfTransformer()\n",
    "pipe = Pipeline([('count', vectorizer),\n",
    "                 ('tfid', tfidftransformer)])\n",
    "pipe_model = pipe.fit(train_data)\n",
    "\n",
    "train_preds = pipe_model.transform(train_data)\n",
    "train_array = train_preds.toarray()\n",
    "\n",
    "test_preds = pipe_model.transform(test_data)\n",
    "test_array = test_preds.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(79816, 4137)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(train_array, label=train_label)\n",
    "dtest = xgb.DMatrix(test_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:45:10] WARNING: /tmp/build/80754af9/xgboost-split_1619724447847/work/src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-merror:0.29261\n",
      "[1]\ttrain-merror:0.27821\n",
      "[2]\ttrain-merror:0.27131\n",
      "[3]\ttrain-merror:0.26403\n",
      "[4]\ttrain-merror:0.25401\n",
      "[5]\ttrain-merror:0.24590\n",
      "[6]\ttrain-merror:0.23736\n",
      "[7]\ttrain-merror:0.23039\n",
      "[8]\ttrain-merror:0.22361\n",
      "[9]\ttrain-merror:0.21795\n",
      "[10]\ttrain-merror:0.21141\n",
      "[11]\ttrain-merror:0.20715\n",
      "[12]\ttrain-merror:0.20176\n",
      "[13]\ttrain-merror:0.19674\n",
      "[14]\ttrain-merror:0.19219\n",
      "[15]\ttrain-merror:0.18868\n",
      "[16]\ttrain-merror:0.18464\n",
      "[17]\ttrain-merror:0.18132\n",
      "[18]\ttrain-merror:0.17831\n",
      "[19]\ttrain-merror:0.17516\n",
      "[20]\ttrain-merror:0.17252\n",
      "[21]\ttrain-merror:0.16955\n",
      "[22]\ttrain-merror:0.16730\n",
      "[23]\ttrain-merror:0.16460\n",
      "[24]\ttrain-merror:0.16237\n",
      "[25]\ttrain-merror:0.15999\n",
      "[26]\ttrain-merror:0.15750\n",
      "[27]\ttrain-merror:0.15538\n",
      "[28]\ttrain-merror:0.15327\n",
      "[29]\ttrain-merror:0.15159\n",
      "[30]\ttrain-merror:0.14952\n",
      "[31]\ttrain-merror:0.14772\n",
      "[32]\ttrain-merror:0.14612\n",
      "[33]\ttrain-merror:0.14508\n",
      "[34]\ttrain-merror:0.14361\n",
      "[35]\ttrain-merror:0.14220\n",
      "[36]\ttrain-merror:0.14067\n",
      "[37]\ttrain-merror:0.13951\n",
      "[38]\ttrain-merror:0.13770\n",
      "[39]\ttrain-merror:0.13643\n",
      "[40]\ttrain-merror:0.13461\n",
      "[41]\ttrain-merror:0.13347\n",
      "[42]\ttrain-merror:0.13197\n",
      "[43]\ttrain-merror:0.13068\n",
      "[44]\ttrain-merror:0.12946\n",
      "[45]\ttrain-merror:0.12827\n",
      "[46]\ttrain-merror:0.12664\n",
      "[47]\ttrain-merror:0.12510\n",
      "[48]\ttrain-merror:0.12376\n",
      "[49]\ttrain-merror:0.12290\n",
      "[50]\ttrain-merror:0.12168\n",
      "[51]\ttrain-merror:0.12088\n",
      "[52]\ttrain-merror:0.12013\n",
      "[53]\ttrain-merror:0.11956\n",
      "[54]\ttrain-merror:0.11848\n",
      "[55]\ttrain-merror:0.11750\n",
      "[56]\ttrain-merror:0.11629\n",
      "[57]\ttrain-merror:0.11563\n",
      "[58]\ttrain-merror:0.11448\n",
      "[59]\ttrain-merror:0.11367\n",
      "[60]\ttrain-merror:0.11297\n",
      "[61]\ttrain-merror:0.11177\n",
      "[62]\ttrain-merror:0.11101\n",
      "[63]\ttrain-merror:0.11020\n",
      "[64]\ttrain-merror:0.10958\n",
      "[65]\ttrain-merror:0.10884\n",
      "[66]\ttrain-merror:0.10804\n",
      "[67]\ttrain-merror:0.10733\n",
      "[68]\ttrain-merror:0.10673\n",
      "[69]\ttrain-merror:0.10603\n",
      "[70]\ttrain-merror:0.10543\n",
      "[71]\ttrain-merror:0.10462\n",
      "[72]\ttrain-merror:0.10361\n",
      "[73]\ttrain-merror:0.10301\n",
      "[74]\ttrain-merror:0.10229\n",
      "[75]\ttrain-merror:0.10131\n",
      "[76]\ttrain-merror:0.10042\n",
      "[77]\ttrain-merror:0.09968\n",
      "[78]\ttrain-merror:0.09899\n",
      "[79]\ttrain-merror:0.09853\n",
      "[80]\ttrain-merror:0.09790\n",
      "[81]\ttrain-merror:0.09686\n",
      "[82]\ttrain-merror:0.09617\n",
      "[83]\ttrain-merror:0.09562\n",
      "[84]\ttrain-merror:0.09513\n",
      "[85]\ttrain-merror:0.09435\n",
      "[86]\ttrain-merror:0.09377\n",
      "[87]\ttrain-merror:0.09279\n",
      "[88]\ttrain-merror:0.09223\n",
      "[89]\ttrain-merror:0.09132\n",
      "[90]\ttrain-merror:0.09055\n",
      "[91]\ttrain-merror:0.08997\n",
      "[92]\ttrain-merror:0.08936\n",
      "[93]\ttrain-merror:0.08883\n",
      "[94]\ttrain-merror:0.08803\n",
      "[95]\ttrain-merror:0.08728\n",
      "[96]\ttrain-merror:0.08674\n",
      "[97]\ttrain-merror:0.08627\n",
      "[98]\ttrain-merror:0.08579\n",
      "[99]\ttrain-merror:0.08527\n"
     ]
    }
   ],
   "source": [
    "param = {'max_depth':10, \n",
    "         'eta':0.5, \n",
    "         'eval_metric':'merror', \n",
    "         'silent':1,\n",
    "         'objective':'multi:softmax', \n",
    "         'num_class': 3}\n",
    "evallist  = [(dtrain,'train')]\n",
    "num_round = 30\n",
    "bst = xgb.train(param, dtrain, num_round, evallist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 模型结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = bst.predict(dtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.39      0.47      1796\n",
      "           1       0.70      0.83      0.76      5653\n",
      "           2       0.69      0.57      0.63      2551\n",
      "\n",
      "    accuracy                           0.69     10000\n",
      "   macro avg       0.66      0.60      0.62     10000\n",
      "weighted avg       0.68      0.69      0.68     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = classification_report(test_label, preds)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
