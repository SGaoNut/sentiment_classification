{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 机器学习解决中文文本分类问题\n",
    "\n",
    "![jupyter](./imgs/chinese_nlp.jpg)\n",
    "\n",
    "### 词袋模型/TF-IDF + 分类模型(LR\\SVM\\XGBoost等)\n",
    "### 中文文本分类传统流程：分词 -> 去停用词 -> TF-IDF特征抽取 -> 分类模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.中文分词\n",
    "### https://github.com/fxsjy/jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.738 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default Mode: 武汉/ 加油/ ！/ 中国/ 加油/ ！/ 没有/ 一个/ 冬天/ 不可逾越/ ，/ 没有/ 一个/ 春天/ 不会/ 来临/ 。\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import jieba\n",
    "\n",
    "seg_list = jieba.cut(\"武汉加油！中国加油！没有一个冬天不可逾越，没有一个春天不会来临。\", cut_all=False)\n",
    "print(\"Default Mode: \" + \"/ \".join(seg_list))  # 精确模式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.词频向量化\n",
    "CountVectorizer 类会将文本中的词语转换为词频矩阵  \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['疫情 结束 ， 一起 去 武汉 看 樱花 ！', '可怕 ！ 新冠 疫情 从 武汉 蔓延 ！', '军运会 在 武汉 举行 。', '武汉 加油 ！']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = ['疫情结束，一起去武汉看樱花！',\n",
    "          '可怕！新冠疫情从武汉蔓延！',\n",
    "          '军运会在武汉举行。',\n",
    "          '武汉加油！',\n",
    "        ]\n",
    "\n",
    "# 精准模式进行分词\n",
    "tokenzied_corpus = [' '.join(jieba.cut(sentence, cut_all=False)) for sentence in corpus]\n",
    "tokenzied_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('一起', 0), ('举行', 1), ('军运会', 2), ('加油', 3), ('可怕', 4), ('新冠', 5), ('樱花', 6), ('武汉', 7), ('疫情', 8), ('结束', 9), ('蔓延', 10)]\n",
      "\n",
      "（文档索引，词汇索引）文档中的词频\n",
      "  (0, 8)\t1\n",
      "  (0, 9)\t1\n",
      "  (0, 0)\t1\n",
      "  (0, 7)\t1\n",
      "  (0, 6)\t1\n",
      "  (1, 8)\t1\n",
      "  (1, 7)\t1\n",
      "  (1, 4)\t1\n",
      "  (1, 5)\t1\n",
      "  (1, 10)\t1\n",
      "  (2, 7)\t1\n",
      "  (2, 2)\t1\n",
      "  (2, 1)\t1\n",
      "  (3, 7)\t1\n",
      "  (3, 3)\t1\n"
     ]
    }
   ],
   "source": [
    "# 定义词频矩阵提取器\n",
    "vectorizer = CountVectorizer(min_df=1) \n",
    "# min_df: df为document frequence即词出现在所有文档中的次数，\n",
    "# 最小文档频率意思为构造词频矩阵时选取的词汇要最少要出现在几篇文档内\n",
    "\n",
    "X = vectorizer.fit_transform(tokenzied_corpus)  # 将词频矩阵提取器应用在分词后的语料\n",
    "feature_name = vectorizer.get_feature_names()\n",
    "print([(word, index) for (index, word) in enumerate(feature_name)])\n",
    "print()\n",
    "print(\"（文档索引，词汇索引）文档中的词频\")\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\t0\t0\t0\t0\t0\t1\t1\t1\t1\t0\n",
      "0\t0\t0\t0\t1\t1\t0\t1\t1\t0\t1\n",
      "0\t1\t1\t0\t0\t0\t0\t1\t0\t0\t0\n",
      "0\t0\t0\t1\t0\t0\t0\t1\t0\t0\t0\n"
     ]
    }
   ],
   "source": [
    "#将词频稀疏矩阵转化为一般矩阵\n",
    "for w in X.toarray():\n",
    "    print('\\t'.join([str(round(x, 2)) for x in w]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.TF-IDF特征\n",
    "\n",
    "有些词在文本中尽管词频高，但是并不重要，这个时候就可以用TF-IDF技术。  \n",
    "某一特定文件内的高词语频率，以及该词语在整个文件集合中的低文件频率，可以产生出高权重的TF-IDF。   \n",
    "因此，TF-IDF倾向于过滤掉常见的词语，保留重要的词语。 \n",
    "\n",
    "这里使用TfidfTransformer将前面的词频矩阵转化为TF-IDF表示\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['一起', '举行', '军运会', '加油', '可怕', '新冠', '樱花', '武汉', '疫情', '结束', '蔓延']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer \n",
    "\n",
    "# TfidfTransformer在CountVectorizer提取的词频矩阵基础上构造，所以这里定义一个pipeline，将过程串起来\n",
    "pipeline = Pipeline([('count', CountVectorizer()),\n",
    "                 ('tfid', TfidfTransformer())])\n",
    "\n",
    "# 训练pipeline\n",
    "pipeline_model = pipeline.fit(tokenzied_corpus)\n",
    "\n",
    "# 查看pipeline model中词频矩阵中的词汇，即TF中的term\n",
    "print(pipeline_model['count'].get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.92\t1.92\t1.92\t1.92\t1.92\t1.92\t1.92\t1.0\t1.51\t1.92\t1.92\n"
     ]
    }
   ],
   "source": [
    "# 查看pipeline model中IDF,即逆词频\n",
    "\n",
    "idfs = pipeline_model['tfid'].idf_\n",
    "print('\\t'.join([str(round(x, 2)) for x in idfs]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.51\t0.0\t0.0\t0.0\t0.0\t0.0\t0.51\t0.26\t0.4\t0.51\t0.0\n",
      "0.0\t0.0\t0.0\t0.0\t0.51\t0.51\t0.0\t0.26\t0.4\t0.0\t0.51\n",
      "0.0\t0.66\t0.66\t0.0\t0.0\t0.0\t0.0\t0.35\t0.0\t0.0\t0.0\n",
      "0.0\t0.0\t0.0\t0.89\t0.0\t0.0\t0.0\t0.46\t0.0\t0.0\t0.0\n"
     ]
    }
   ],
   "source": [
    "# 对文本进行tfidf特征抽取\n",
    "featrure = pipeline_model.transform(tokenzied_corpus)\n",
    "# 查看tfidf矩阵\n",
    "# weight就是模型的输入，它的大小(m,n),m是文本个数，n是词库的大小\n",
    "weight = featrure.toarray()  # 将稀疏矩阵转化为一般矩阵，方便查看\n",
    "for w in weight:\n",
    "    print('\\t'.join([str(round(x, 2)) for x in w]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. XGBoost模型\n",
    "\n",
    "说到XGBoost，不得不提GBDT(Gradient Boosting Decision Tree)。因为XGBoost本质上还是一个GBDT，但是力争把速度和效率发挥到极致，所以叫X (Extreme) GBoosted。高效地实现了GBDT算法并进行了算法和工程上的许多改进。\n",
    "- 对于非深度学习类型的机器学习分类问题，XGBoost是最流行的库。\n",
    "- 由于XGBoost可以很好地扩展到大型数据集中，并支持多种语言，它在商业化环境中特别有用。\n",
    "- XGBoost不仅学习效果很好，而且速度也很快，相比梯度提升算法在另一个常用机器学习库scikit-learn中的实现，XGBoost的性能经常有十倍以上的提升。\n",
    "\n",
    "https://xgboost.readthedocs.io/en/latest/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 数据处理：分词、去停用词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['--', '?', '“', '”', '》', '－－', 'able', 'about', 'above', 'according']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载停用词典，可以自定义\n",
    "with open('./data/baidu_stopwords.txt', 'r') as f:\n",
    "    stop_words = f.read().split('\\n')\n",
    "\n",
    "stop_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'武汉 樱花 很漂亮'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 分词 -> 去停用词 -> 清除无用词\n",
    "\n",
    "def cut_and_clean(text, stop_words):\n",
    "    cuted_text = ' '.join([x for x in jieba.lcut(text) if x not in stop_words])\n",
    "    clean_text = re.sub('([\\.!\\/_,?=\\$%\\^\\)*\\(\\+\\\"\\'\\+——！:：；，。？、~@#%……&*（）·¥\\-\\|\\\\《》〈〉～《 》「」『』\\{\\}\\|])', ' ', cuted_text)\n",
    "    clean_text = re.sub('\\s{2,}', ' ', clean_text)\n",
    "    clean_text = clean_text.lower()\n",
    "    return clean_text.strip()\n",
    "\n",
    "text = '武汉的樱花很漂亮！'\n",
    "cut_and_clean(text, stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>肺炎疫情</td>\n",
       "      <td>1</td>\n",
       "      <td>肺炎 疫情</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>【西方记者挑事提问，WHO总干事用事实力挺中国】12日，世界卫生组织大会上，有西方记者挑事提...</td>\n",
       "      <td>1</td>\n",
       "      <td>【 西方 记者 挑事 提问 who 总干事 用事 实力 挺 中国 】 12 日 世界卫生组织...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>人数终于 始呈下降趋势。这个春节有喜有忧，但事情终有完结的那一天。中午出去买了一次蔬菜，过了...</td>\n",
       "      <td>1</td>\n",
       "      <td>人数 终于 始 呈 下降 趋势 春节 有喜有忧 事情 终 完结 中午 买 蔬菜 六道 关卡 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>【抗病毒小刘上线】今天有好几个朋友来问我戴没戴口罩要不要寄口罩给我，超级感动，小刘除了被吓到...</td>\n",
       "      <td>1</td>\n",
       "      <td>【 抗病毒 小 刘 上线 】 好几个 朋友 问 戴 没 戴 口罩 寄 口罩 超级 感动 小 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>潜伏期传染，后期会不会更惊讶O最新疫情地图出炉，这传播速度有让你觉得惊讶吗</td>\n",
       "      <td>1</td>\n",
       "      <td>潜伏期 传染 后期 会 更 惊讶 o 最新 疫情 地图 出炉 传播速度 惊讶</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label  \\\n",
       "0                                               肺炎疫情      1   \n",
       "1  【西方记者挑事提问，WHO总干事用事实力挺中国】12日，世界卫生组织大会上，有西方记者挑事提...      1   \n",
       "2  人数终于 始呈下降趋势。这个春节有喜有忧，但事情终有完结的那一天。中午出去买了一次蔬菜，过了...      1   \n",
       "3  【抗病毒小刘上线】今天有好几个朋友来问我戴没戴口罩要不要寄口罩给我，超级感动，小刘除了被吓到...      1   \n",
       "4              潜伏期传染，后期会不会更惊讶O最新疫情地图出炉，这传播速度有让你觉得惊讶吗      1   \n",
       "\n",
       "                                      processed_text  \n",
       "0                                              肺炎 疫情  \n",
       "1  【 西方 记者 挑事 提问 who 总干事 用事 实力 挺 中国 】 12 日 世界卫生组织...  \n",
       "2  人数 终于 始 呈 下降 趋势 春节 有喜有忧 事情 终 完结 中午 买 蔬菜 六道 关卡 ...  \n",
       "3  【 抗病毒 小 刘 上线 】 好几个 朋友 问 戴 没 戴 口罩 寄 口罩 超级 感动 小 ...  \n",
       "4             潜伏期 传染 后期 会 更 惊讶 o 最新 疫情 地图 出炉 传播速度 惊讶  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 对训练集数据进行处理\n",
    "\n",
    "train_df = pd.read_csv('./processed_data/train.csv', encoding='utf-8')\n",
    "train_df['processed_text'] = train_df['text'].apply(lambda text: cut_and_clean(text, stop_words))\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>星火线上课 足不出户，同样进步！L星火教育呼市的 视频</td>\n",
       "      <td>2</td>\n",
       "      <td>星火 线 上课 足不出户 l 星火 教育 呼市 视频</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>中科院病毒研究所石正丽老师演讲《追踪SARS的源头》探索病毒与人类社会的关系。真是好看</td>\n",
       "      <td>2</td>\n",
       "      <td>中科院 病毒 研究所 石正丽 老师 演讲 追踪 sars 源头 探索 病毒 人类 社会 关系 好看</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>嗯 @熊中默就是那个给那位挂基层辅警照片 暴平安天门几万条艾滋病巨婴gay洗地还挂plmm照...</td>\n",
       "      <td>0</td>\n",
       "      <td>熊中默 那位 挂 基层 辅警 照片 暴 平安 天门 几万 条 艾滋病 巨婴 gay 洗地 还...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>今天看到两条让我真的笑，笑出声的 。「为防病毒，出门抢购双黄连，染上病毒。」O 「新冠病毒感...</td>\n",
       "      <td>0</td>\n",
       "      <td>两条 真的 笑 笑 出声 防病毒 出门 抢购 双黄连 染上 病毒 o 新冠 病毒感染 肾脏 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@_莫西干_</td>\n",
       "      <td>1</td>\n",
       "      <td>莫西 干</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label  \\\n",
       "0                        星火线上课 足不出户，同样进步！L星火教育呼市的 视频      2   \n",
       "1        中科院病毒研究所石正丽老师演讲《追踪SARS的源头》探索病毒与人类社会的关系。真是好看      2   \n",
       "2  嗯 @熊中默就是那个给那位挂基层辅警照片 暴平安天门几万条艾滋病巨婴gay洗地还挂plmm照...      0   \n",
       "3  今天看到两条让我真的笑，笑出声的 。「为防病毒，出门抢购双黄连，染上病毒。」O 「新冠病毒感...      0   \n",
       "4                                             @_莫西干_      1   \n",
       "\n",
       "                                      processed_text  \n",
       "0                         星火 线 上课 足不出户 l 星火 教育 呼市 视频  \n",
       "1  中科院 病毒 研究所 石正丽 老师 演讲 追踪 sars 源头 探索 病毒 人类 社会 关系 好看  \n",
       "2  熊中默 那位 挂 基层 辅警 照片 暴 平安 天门 几万 条 艾滋病 巨婴 gay 洗地 还...  \n",
       "3  两条 真的 笑 笑 出声 防病毒 出门 抢购 双黄连 染上 病毒 o 新冠 病毒感染 肾脏 ...  \n",
       "4                                               莫西 干  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 对测试集数据进行处理\n",
    "\n",
    "test_df = pd.read_csv('./processed_data/test.csv', encoding='utf-8')\n",
    "test_df['processed_text'] = test_df['text'].apply(lambda text: cut_and_clean(text, stop_words))\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提取文本与标签\n",
    "\n",
    "train_data, train_label = train_df['processed_text'], train_df['label']\n",
    "\n",
    "test_data, test_label = test_df['processed_text'], test_df['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 2 文本特征化：词频 -> TF-IDF -> xgb Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tfidf 特征向量维度为: 4013\n"
     ]
    }
   ],
   "source": [
    "# 词频矩阵提取器\n",
    "vectorizer = CountVectorizer(max_df=1000,  # 最大文档频率：词汇最多出现在多少篇文档中\n",
    "                             min_df=50)    # 最小文档频率：词汇最少出现在多少篇文档中\n",
    "# Tfidf提取器\n",
    "tfidftransformer = TfidfTransformer()\n",
    "\n",
    "# 定义特征提取pipeline\n",
    "pipe = Pipeline([('count', vectorizer),\n",
    "                 ('tfid', tfidftransformer)])\n",
    "\n",
    "# 在训练集上进行pipeline构造\n",
    "pipe_model = pipe.fit(train_data)\n",
    "\n",
    "# 查看tfidf特征向量维度\n",
    "print(f\"Tfidf 特征向量维度为: {len(pipe_model['tfid'].idf_)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./xgb/pipe.pkl']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 保存pipeline model\n",
    "pipe_save_path = './xgb/pipe.pkl'\n",
    "joblib.dump(pipe_model, pipe_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对训练与测试集文本进行tfidf特征抽取\n",
    "train_preds = pipe_model.transform(train_data)\n",
    "train_array = train_preds.toarray()\n",
    "\n",
    "test_preds = pipe_model.transform(test_data)\n",
    "test_array = test_preds.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(79816, 4013)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看训练集特征向量形状\n",
    "train_array.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost数据集\n",
    "\n",
    "在XGBoost中数据的加载是使用其特有的数据格式进行训练的，即xgb.DMatrix。   \n",
    "之前说过样本分布不均衡问题，这里我们在构建数据集时通过对样本权重的加权，提高样本少类别的权重。   \n",
    "xgb.DMatrix设置每一个样本的权重，这样模型在计算损失的过程中都会结合每个样本的权重去计算。   \n",
    "这里我们提高类别较少的0，2类别的权重。  \n",
    "##### Loss = 0.2 * Loss0 + 0.1 * Loss1+ 0.2 * Loss2   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.1, 0.1, 0.1, 0.1, 0.1, 0.2, 0.1, 0.2, 0.2, 0.1]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_weight_dict = {\n",
    "    0: 0.2,\n",
    "    1: 0.1,\n",
    "    2: 0.2\n",
    "}\n",
    "\n",
    "sample_weight = [label_weight_dict[label] for label in train_label]\n",
    "sample_weight[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将训练与测试数据转化为xgboost的DMatrix形式，并对样本重要性进行加权\n",
    "dtrain = xgb.DMatrix(train_array, label=train_label, weight=sample_weight)\n",
    "dtest = xgb.DMatrix(test_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 模型训练\n",
    "\n",
    "XGBoost参数调优完全指南：https://blog.csdn.net/han_xiaoyang/article/details/52665396   \n",
    "XGBoost 重要参数(调参使用): https://www.cnblogs.com/TimVerion/p/11436001.html   \n",
    "\n",
    "XGBoost的训练API：https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.training   \n",
    "包括： \n",
    "- xgboost.train(params, dtrain, num_boost_round=10, evals=(), obj=None, feval=None, maximize=None, early_stopping_rounds=None, evals_result=None, verbose_eval=True, xgb_model=None, callbacks=None)¶    \n",
    "\n",
    "- xgboost.cv(params, dtrain, num_boost_round=10, nfold=3, stratified=False, folds=None, metrics=(), obj=None, feval=None, maximize=None, early_stopping_rounds=None, fpreproc=None, as_pandas=True, verbose_eval=None, show_stdv=True, seed=0, callbacks=None, shuffle=True)\n",
    "\n",
    "可以先通过xgboost.cv进行交叉验证选出最合适的num_boost_round，在通过xgboost.train进行训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param = {'max_depth':6, \n",
    "#          'eta':0.1, \n",
    "#          'eval_metric':'merror', \n",
    "#          'objective':'multi:softmax', \n",
    "#          'num_class': 3}\n",
    "\n",
    "# max_n_estimators = 1000\n",
    "\n",
    "# cv_res= xgb.cv(param,\n",
    "#                dtrain,\n",
    "#                num_boost_round=max_n_estimators,  # 最大迭代次数\n",
    "#                early_stopping_rounds=20, # 没有提升迭代停止，输出最好的轮数\n",
    "#                nfold=5,  # n折交叉\n",
    "#                metrics='auc',  # 评价指标\n",
    "#                show_stdv=True,  # 打印交叉验证的标准差\n",
    "#                verbose_eval=10 #每10轮打印指标\n",
    "#                )\n",
    "\n",
    "# cv_res\n",
    "\n",
    "# #cv_res.shape[0]为最佳迭代次数\n",
    "# best_num_boost_round = cv_res.shape[0] \n",
    "\n",
    "# bst = xgb.train(param,\n",
    "#                 dtrain,\n",
    "#                 num_boost_round=best_num_boost_round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "param = {'max_depth': 6,  # 树的最大深度，一般3-10\n",
    "         'eta':0.1,  # learning rate每一步迭代的步长，很重要。太大了运行准确率不高，太小了运行速度慢，一般0.01-0.2\n",
    "         'eval_metric':'merror', # 评估度量方法，merror对应多分类错误率\n",
    "         'objective':'multi:softmax', # 损失函数，这里设置为使用softmax的多分类器，下面还需设置num_class\n",
    "         'num_class': 3}\n",
    "\n",
    "best_num_boost_round = 100 # 生成的最大树的数目，也是最大的迭代次数, 这里通过cross validation得到最优的num_boost_round\n",
    "\n",
    "# XGBoost模型训练\n",
    "bst = xgb.train(param,\n",
    "                dtrain,\n",
    "                num_boost_round=best_num_boost_round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存训练好的模型\n",
    "model_save_path = './xgb/xgb.json'\n",
    "bst.save_model(model_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 模型评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型预测\n",
    "preds = bst.predict(dtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.30      0.39      1796\n",
      "           1       0.66      0.82      0.73      5653\n",
      "           2       0.61      0.47      0.53      2551\n",
      "\n",
      "    accuracy                           0.64     10000\n",
      "   macro avg       0.61      0.53      0.55     10000\n",
      "weighted avg       0.63      0.64      0.62     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 评估分类结果\n",
    "result = classification_report(test_label, preds)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 模型预测\n",
    "### 数据预处理-> 分词 -> 去停用词 -> TF-IDF特征抽取 -> xgb模型预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载停用词\n",
    "with open('./data/baidu_stopwords.txt', 'r') as f:\n",
    "    stop_words = f.read().split('\\n')\n",
    "\n",
    "#加载特征抽取模型\n",
    "pipe_save_path = './xgb/pipe.pkl'\n",
    "pipe_model = joblib.load(pipe_save_path)\n",
    "\n",
    "#加载机器学习模型\n",
    "model_save_path = './xgb/xgb.json'\n",
    "xgb_model = xgb.Booster()\n",
    "xgb_model.load_model(model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['疫情 被困 家里 2 月 好 压抑 感觉 抑郁', '我国 一个 新冠 病毒 疫苗 获批 紧急', '打赢 这场 仗 抗击 新馆 疫情']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentences = [\n",
    "    \"因为疫情被困家里2个月了，好压抑啊，感觉自己抑郁了！\",\n",
    "    \"我国又一个新冠病毒疫苗获批紧急使用。\",\n",
    "    \"我们在一起，打赢这场仗，抗击新馆疫情，我们在行动！\"]\n",
    "\n",
    "# 进行分词与预处理\n",
    "predict_tokenizes = [cut_and_clean(sentence, stop_words) for sentence in predict_sentences]\n",
    "\n",
    "predict_tokenizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 提取TF-IDF特征\n",
    "features_array = pipe_model.transform(predict_tokenizes).toarray()\n",
    "features_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 1]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 转化为xgb的DMatrix\n",
    "dpredict = xgb.DMatrix(features_array)\n",
    "\n",
    "# 使用模型进行预测\n",
    "predict_result = xgb_model.predict(dpredict)\n",
    "\n",
    "# 将标签还原为[-1, 0, 1]\n",
    "predict_result = [int(label - 1) for label in predict_result]\n",
    "predict_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: 因为疫情被困家里2个月了，好压抑啊，感觉自己抑郁了！\n",
      "Predict: 0\n",
      "Text: 我国又一个新冠病毒疫苗获批紧急使用。\n",
      "Predict: 0\n",
      "Text: 我们在一起，打赢这场仗，抗击新馆疫情，我们在行动！\n",
      "Predict: 1\n"
     ]
    }
   ],
   "source": [
    "# 展示结果\n",
    "for text, label in zip(predict_sentences, predict_result):\n",
    "    print(f'Text: {text}\\nPredict: {label}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
